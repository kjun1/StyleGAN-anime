{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"b79d73a9-5b7a-4560-90c7-de031bc59fe6","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"cells":[{"cell_type":"code","source":"#!g1.1\nfrom torch.profiler import profile, record_function, ProfilerActivity\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torch.autograd import grad, Variable\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchvision\nimport torch\n\nfrom IPython.display import clear_output, HTML, display\nfrom matplotlib import animation, pyplot as plt\nfrom time import gmtime, strftime\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport wandb\nimport os","metadata":{"cellId":"vohsfoq60md71ib729jn7","trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#!g1.1\n!rm -rf 'Weight/StyleGAN R1 64'","metadata":{"cellId":"xdqh3cp4eof2osf9pgtc6c","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#!g1.1\nfrom start import init_train\n\nTrainer = init_train(\"configs/StyleGAN.json\", True)","metadata":{"cellId":"zal4b1oam1rcuspg9rlgg","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Generator(\n  (mapping): Mapping(\n    (blocks): ModuleList(\n      (0): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (1): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (2): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (3): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (4): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (5): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (6): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n      (7): Sequential(\n        (0): Linear(in_features=256, out_features=256, bias=True)\n        (1): LeakyReLU(negative_slope=0.2)\n      )\n    )\n  )\n  (to_rgb): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n  (blocks): ModuleDict(\n    (res 4): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n    )\n    (res 8): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n    )\n    (res 16): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n    )\n    (res 32): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n    )\n    (res 64): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=1024, bias=True)\n      )\n      (Conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=512, bias=True)\n      )\n    )\n    (res 128): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=512, bias=True)\n      )\n      (Conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=256, bias=True)\n      )\n    )\n    (res 256): BlockG(\n      (up_sample): Upsample(scale_factor=2.0, mode=bilinear)\n      (Conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (AdaIN1): AdaIN(\n        (A): Linear(in_features=256, out_features=256, bias=True)\n      )\n      (Conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (AdaIN2): AdaIN(\n        (A): Linear(in_features=256, out_features=128, bias=True)\n      )\n    )\n  )\n)\nDiscriminator(\n  (fromRGB): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n  (Linear): Linear(in_features=8192, out_features=1, bias=True)\n  (blocks): ModuleDict(\n    (res 256): BlockD(\n      (Conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (Conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down_sample): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (res 128): BlockD(\n      (Conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (Conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down_sample): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (res 64): BlockD(\n      (Conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down_sample): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (res 32): BlockD(\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down_sample): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (res 16): BlockD(\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down_sample): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n    (res 8): BlockD(\n      (Conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (Act): LeakyReLU(negative_slope=0.2)\n      (Conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (down_sample): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    )\n  )\n)\n"},{"output_type":"stream","name":"stderr","text":" 41%|████      | 37987/92219 [03:21<04:20, 208.57it/s]"},{"output_type":"error","ename":"Error","evalue":"connection with executor has been lost. Please try again.","traceback":[]}],"execution_count":12},{"cell_type":"code","source":"#!g1.1\n!cp trainer.py starter.ipynb start.py losses.py -r utils models configs Project/\n!tar -cvf project.tar Project/","metadata":{"cellId":"0438yy6iedexsatxq0edlzm","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Project/\nProject/trainer.py\nProject/starter.ipynb\nProject/start.py\nProject/losses.py\nProject/utils/\nProject/utils/.ipynb_checkpoints/\nProject/utils/register.py\nProject/utils/__pycache__/\nProject/utils/__pycache__/utils.cpython-37.pyc\nProject/utils/__pycache__/register.cpython-37.pyc\nProject/utils/__pycache__/images.cpython-37.pyc\nProject/utils/images.py\nProject/models/\nProject/models/StyleGAN.py\nProject/models/.ipynb_checkpoints/\nProject/models/__pycache__/\nProject/models/__pycache__/R1GAN.cpython-37.pyc\nProject/models/__pycache__/StyleGAN_fix.cpython-37.pyc\nProject/models/__pycache__/StyleGAN.cpython-37.pyc\nProject/models/R1GAN.py\nProject/models/StyleGAN_fix.py\nProject/configs/\nProject/configs/StyleGAN.json\nProject/configs/.ipynb_checkpoints/\nProject/configs/R1GAN.json\n"}],"execution_count":16},{"cell_type":"code","source":"#!g1.1\nTrainer.train_loop()","metadata":{"cellId":"n1qrpzns7xr49nn3gp84ol","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nnext(iter(Trainer.dataloader))[0].shape","metadata":{"cellId":"p9i2jcoxwhqceqcy715niu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nfrom utils.images import TensorToImage\nrandom_img = lambda: int(np.random.rand(1)[0] * len(dataset))\nplot_width, plot_height = 10, 5\n\nfig = plt.figure(figsize=(3 * plot_width, 3 * plot_height))\nfor sample_n in range(1, plot_width * plot_height + 1):\n    ax = fig.add_subplot(plot_height, plot_width, sample_n)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.imshow(TensorToImage(next(iter(Trainer.dataloader))[sample_n].detach().cpu(), 0.5, 0.375))","metadata":{"cellId":"56qpqx1mxx6qd9vqxscoe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nstate = torch.load('Weight/StyleGAN StyleGAN 64/weight 62.pth')","metadata":{"cellId":"sryg044bm5ri1bdsbffm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nstate['D'].pop('fromRGB.weight', None)\nstate['D'].pop('fromRGB.bias', None)\nstate['D'].pop('Linear.weight', None)","metadata":{"cellId":"tf3nfmrg4id4vpnjs6nwv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nTrainer.D.load_state_dict(state['D'], strict=False)","metadata":{"cellId":"3kkwfo2r20qrjsplgnnhmj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nstate['G'].pop('to_rgb.weight', None)","metadata":{"cellId":"fh0r6nz1o5lg9pgdrnaj0q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nTrainer.G.load_state_dict(state['G'], strict=False)","metadata":{"cellId":"2gpgg0zfqk6nejgk55pjnc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nTrainer.train_loop()","metadata":{"cellId":"g9qz1tkhji9bvq5w8k966o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nnext(iter(Trainer.dataloader))[0].shape","metadata":{"cellId":"y75zdshpfykzwlrf51hb8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nfrom utils.images import TensorToImage\n\nrandom_img = lambda: int(np.random.rand(1)[0] * len(dataset))\nplot_width, plot_height = 10, 10\n\nfig = plt.figure(figsize=(3 * plot_width, 3 * plot_height))\nfor sample_n in range(1, plot_width * plot_height + 1):\n    ax = fig.add_subplot(plot_height, plot_width, sample_n)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.imshow(TensorToImage(Trainer.generate_images().view(3, 128, 128).detach().cpu(), 0.5, 0.375))","metadata":{"cellId":"ditqfoo0234igfy4yg58q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nplt.imshow(TensorToImage(Trainer.generate_images().view(3, 128, 128).detach().cpu(), 0.5, 0.375))","metadata":{"cellId":"3jxcydldg6wn96ot2j4mgn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nw = Trainer.G.mapping(torch.randn((1, 128), device=Trainer.device))\nimg = Trainer.G.const.expand(w.size(0), -1, -1, -1)","metadata":{"cellId":"owowe6r17r95y82qh3ntc2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nlen(Trainer.G.blocks)","metadata":{"cellId":"vaqhaohck8krfmot3a0f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nimg2 = Trainer.G.blocks[0](img, w)\nplt.imshow(TensorToImage(img2.view(3, 4, 4).detach().cpu(), 0.5, 0.375))","metadata":{"cellId":"89yhslc6vkpwdkpm5yytt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n!python3 start.py \"configs/StyleGAN.json\" True","metadata":{"cellId":"gzfncr3eckd33uytmc5w0d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndef generate_video(samples_list, delay=1000):\n    def process_image(img):\n        if len(img.shape) == 4:\n            img = img[0]\n        plt.axis('off')\n        return [plt.imshow(TensorToImage(img, 0.5, 0.28))]\n\n    fig = plt.figure(figsize=(8, 8))\n    plt.axis('off')\n    frames = [process_image(elem) for elem in samples_list]\n    \n    ani = animation.ArtistAnimation(fig, frames, interval=delay)  \n    display(HTML(ani.to_html5_video()))","metadata":{"cellId":"yv0qr6bvhvrkibo70716gr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nfrom start import init_train\n\nTrainer = init_train(\"configs/StyleGAN.json\", True, load_dataset=True)\nTrainer.train_loop()","metadata":{"cellId":"8lvds62ndmnxkec1ouldu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"myrgiglxa9hpnugiylv95","trusted":true},"outputs":[],"execution_count":null}]}